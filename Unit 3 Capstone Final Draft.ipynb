{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing the Market vs Time in the Market\n",
    "   ## Comparison of Supervised Learning Models to a Simulated 401(k) Approach\n",
    "  \n",
    "  ### Unit 3 Capstone Project\n",
    "  ### Matthew Kennedy, August 2017\n",
    "\n",
    "   ## Section 1: Overview of Dataset and Analysis of Data\n",
    "   \n",
    "   The dataset used in this project comes from Kaggle user \"CNuge.\" The dataset contains historical stock prices over the last five years for all companies in the S&P 500 index and can be found at https://www.kaggle.com/camnugent/sandp500. This project will use the files that have the historical prices for individual stocks.   \n",
    "       \n",
    "   The dataset contains the following columns: \n",
    "       \n",
    "       Date - In the format of yy-mm-dd\n",
    "       Open - Price of the stock in USD at market open\n",
    "       High - Highest price reached in the day\n",
    "       Low - Lowest price reached in the day\n",
    "       Close - The price the stock had at the end of the day\n",
    "       Volume - Number of shares traded\n",
    "       Name - The stock's ticker name\n",
    "       \n",
    "   The user collected the data by using the python library, 'pandas_datareader,' to scrape Google Finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'D:\\\\Data\\\\sandp500\\\\GOOGL_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e5249f7a9b81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;31m#original = pd.read_csv('C:\\\\Users\\\\mkennedy\\\\sandp500\\\\individual_stocks_5yr\\\\GOOGL_data.csv', encoding='utf-8-sig')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;31m# This is the filepath on my desktop:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moriginal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\\\Data\\\\sandp500\\\\GOOGL_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\mkennedy\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\mkennedy\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\mkennedy\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\mkennedy\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\mkennedy\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'D:\\\\Data\\\\sandp500\\\\GOOGL_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# This project will use the historical prices of GOOGL\n",
    "\n",
    "# Read the Dataset, store the original\n",
    "# This is the filepath on my laptop:\n",
    "#original = pd.read_csv('C:\\\\Users\\\\mkennedy\\\\sandp500\\\\individual_stocks_5yr\\\\GOOGL_data.csv', encoding='utf-8-sig')\n",
    "# This is the filepath on my desktop:\n",
    "original = pd.read_csv('D:\\\\Data\\\\sandp500\\\\GOOGL_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copy a dataframe of the original data to manipulate\n",
    "data = original\n",
    "\n",
    "# Print the headers of the dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the footer to make sure there are no rows of text\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no footers that need to be excluded.\n",
    "There are 1257 rows of stock data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The describe method provides some additional information about the data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The dtypes call will display the data types. \n",
    "# This is used to make sure all numerical values have the correct data type to work with in the models.\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset appears to be clean and easy to work with.\n",
    "\n",
    "Observe the correlations between columns by using seaborn's heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a heatmap to compare the correlation of the columns.\n",
    "import seaborn as sns\n",
    "\n",
    "corrmat = data.corr()\n",
    "\n",
    "# Set up the matplotlib figure.\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "# Draw the heatmap using seaborn\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the values are highly correlated. Creating prediction models based off of time-series data will not be helpful in reaching the goal of this project (to determine whether to buy or sell the at the next opening day). \n",
    "\n",
    "Features will need to be created to generate accurate predictions from the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The momentum will show how many days in a row the stock has moved up or down. \n",
    "\n",
    "# Create a list to store the momentum\n",
    "momentum = [0]\n",
    "i=1\n",
    "# Calculate the momentums and store them in the new column, 'Momentum'\n",
    "for row in data['Close']:\n",
    "    if i < len(data):\n",
    "        if data.Close[i] > data.Close[i-1]:\n",
    "            momentum.append(+1)\n",
    "            i = i+1\n",
    "        elif data.Close[i] < data.Close[i-1]:\n",
    "            momentum.append(-1)\n",
    "            i = i+1\n",
    "\n",
    "data['Momentum'] = momentum\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out the total momentum and the average momentum across all rows.\n",
    "total_mom = sum(data.Momentum)\n",
    "print(total_mom)\n",
    "ave_mom = data.Momentum.mean()\n",
    "print(ave_mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "streak = [0] * len(data)\n",
    "i=1\n",
    "# Calculate the streaks and store them in the new column, 'Streak'\n",
    "for row in data['Close']:\n",
    "    if i < len(data):\n",
    "        if data.Close[i] > data.Close[i-1]:\n",
    "            if streak[i-1] >= 0:\n",
    "                streak[i] = streak[i-1]+1\n",
    "                i = i+1\n",
    "            else:\n",
    "                streak[i]=0\n",
    "                i = i+1\n",
    "        elif data.Close[i] < data.Close[i-1]:\n",
    "            if streak[i-1] <= 0:\n",
    "                streak[i] = streak[i-1]-1\n",
    "                i = i+1\n",
    "            else:\n",
    "                streak[i]=0\n",
    "                i = i+1\n",
    "\n",
    "data['Streak'] = streak\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a 'Future Momentum' feature that the model will attempt to predict.\n",
    "data['Future Momentum'] = data.Momentum.shift(-1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the last row to get rid of the NaN values\n",
    "data = data.iloc[:len(data)-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the tail to make sure the data looks good\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a heatmap to compare the correlation of the columns.\n",
    "import seaborn as sns\n",
    "\n",
    "corrmat = data.corr()\n",
    "\n",
    "# Set up the matplotlib figure.\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "# Draw the heatmap using seaborn\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the change in GOOGL stock prices over time\n",
    "plt = plot(data.Close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Creation and Comparison of Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been analyzed to ensure it can be manipulated, it is time to create some predictive models. For comparison, the scores from the models will be stored in a new table, titled \"Model Comparison.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a table to store the scores for each model.\n",
    "# Title: Model Comparison\n",
    "# Columns: Model, R^2, Accuracy, AUROC\n",
    "# Model values: Linear Regression, Ridge Regression, Lasso Regression, Support Vector Regression, Gradient Boost Classification\n",
    "models = {'Model':[], 'Scoring Metric':[], 'Scoring Value':[]}\n",
    "columns = models.keys()\n",
    "model_comparison = pd.DataFrame(data=models, columns=columns)\n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the variables. \n",
    "# Use the closing value for Y\n",
    "# Use the new features for X\n",
    "Y = data['Future Momentum']\n",
    "X = data[['Close', 'Volume', 'Momentum', 'Streak']]\n",
    "\n",
    "# Create training and test sets.\n",
    "offset = int(X.shape[0] * 0.8)\n",
    "\n",
    "# Put 90% of the data in the training set.\n",
    "X_train, Y_train = X[:offset], Y[:offset]\n",
    "\n",
    "# And put 10% in the test set.\n",
    "X_test, Y_test = X[offset:], Y[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the average momentum for the test set to use in later analysis:\n",
    "print(Y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Declare a logistic regression classifier.\n",
    "# Larger C's lead to reduced regularization of parameters, but because there are\n",
    "#   few features, the value of C has a trivial effect (tested for many C's)\n",
    "lr = LogisticRegression(C=1e9)\n",
    "\n",
    "# Fit the model.\n",
    "lr.fit(X_train,Y_train)\n",
    "y_pred = lr.fit(X_train,Y_train).predict(X_test)\n",
    "\n",
    "\n",
    "print('Confusion Matrix of the Model:')\n",
    "conf_mat = confusion_matrix(Y_test, y_pred)\n",
    "print(conf_mat)\n",
    "\n",
    "# Use Accuracy for Logistic Regression Scoring Metric\n",
    "cv = cross_val_score(lr, X, Y, cv=10, scoring='accuracy') \n",
    "print('Accuracy Score:')\n",
    "print(cv)\n",
    "# Print the average of the R2 and store it in the table\n",
    "print('Average of the Accuracy Score:')\n",
    "print(cv.mean())\n",
    "\n",
    "# Store the data in the model_comparison table\n",
    "models = {'Model':['Logistic Regression'], 'Scoring Metric':'Accuracy', 'Scoring Value':[cv.mean()]}\n",
    "model_comparison = model_comparison.append(pd.DataFrame(data=models, columns=models.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_pred.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fitting a ridge regression model. Alpha is the regularization\n",
    "# parameter (usually called lambda). As alpha gets larger, parameter\n",
    "# shrinkage grows more pronounced. Note that by convention, the\n",
    "# intercept is not regularized. Since we standardized the data\n",
    "# earlier, the intercept should be equal to zero and can be dropped.\n",
    "\n",
    "ridgeregr = linear_model.Ridge(alpha=10, fit_intercept=False) \n",
    "ridgeregr.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = ridgeregr.fit(X_train,Y_train).predict(X_test)\n",
    "\n",
    "# Print the R2.\n",
    "cv = cross_val_score(ridgeregr, X, Y, cv=10, scoring='r2') \n",
    "print('R2 Score:')\n",
    "print(cv)\n",
    "# Print the average of the R2 and store it in the table\n",
    "print('Average of the R2 Score:')\n",
    "print(cv.mean())\n",
    "\n",
    "# Store the data in the model_comparison table\n",
    "models = {'Model':['Ridge Regression'], 'Scoring Metric':'R2', 'Scoring Value':[cv.mean()]}\n",
    "model_comparison = model_comparison.append(pd.DataFrame(data=models, columns=models.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_pred.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(alpha=.35)\n",
    "lasso.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = lasso.fit(X_train,Y_train).predict(X_test)\n",
    "\n",
    "# Print the R-Squared value and store it in the table\n",
    "print('R-Squared of the model:') \n",
    "score = r2_score(Y_test, y_pred)\n",
    "print(score)\n",
    "\n",
    "# Print the R2.\n",
    "cv = cross_val_score(lasso, X, Y, cv=10, scoring='r2') \n",
    "print('R2 Score:')\n",
    "print(cv)\n",
    "# Print the average of the AUROC and store it in the table\n",
    "print('Average of the R2 Score:')\n",
    "print(cv.mean())\n",
    "\n",
    "# Store the data in the model_comparison table\n",
    "models = {'Model':['Lasso Regression'], 'Scoring Metric':'R2', 'Scoring Value':[cv.mean()]}\n",
    "model_comparison = model_comparison.append(pd.DataFrame(data=models, columns=models.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_pred.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Changing value for epsilon may reduce the overfitting\n",
    "\n",
    "# Make a model using SVR here\n",
    "svr = SVR(epsilon=.5)\n",
    "svr.fit(X_train,Y_train)\n",
    "y_pred = svr.fit(X_train,Y_train).predict(X_test)\n",
    "\n",
    "# Use Accuracy for the Scoring Metric\n",
    "cv = cross_val_score(svr, X, Y, cv=10, scoring='r2') \n",
    "print('R2 Score:')\n",
    "print(cv)\n",
    "# Print the average of the AUROC and store it in the table\n",
    "print('Average of the R2 Score:')\n",
    "print(cv.mean())\n",
    "\n",
    "# Store the data in the model_comparison table\n",
    "models = {'Model':['Support Vector Regression'], 'Scoring Metric':'R2', 'Scoring Value':[cv.mean()]}\n",
    "model_comparison = model_comparison.append(pd.DataFrame(data=models, columns=models.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_pred.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use GridSearchCV to determine the best gamma and C values for SVC.\n",
    "# C options: 0.01-1.0, default is 1.0\n",
    "# Kernel types: ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’, default is 'rbf'\n",
    "# Gamma types: float, default is 1/n\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'gamma':[0.1,1], 'C':[0.1,1]}\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc = SVC(gamma = .9)\n",
    "svc.fit(X_train,Y_train)\n",
    "y_pred = svc.fit(X_train,Y_train).predict(X_test)\n",
    "\n",
    "print('Confusion Matrix of the model:')\n",
    "conf_mat = confusion_matrix(Y_test, y_pred)\n",
    "print(conf_mat)\n",
    "\n",
    "# Print the Accuracy.\n",
    "cv = cross_val_score(svc, X, Y, cv=10, scoring='accuracy') \n",
    "print('Accuracy Score:')\n",
    "print(cv)\n",
    "# Print the average of the AUROC and store it in the table\n",
    "print('Average of the Accuracy Score:')\n",
    "print(cv.mean())\n",
    "\n",
    "\n",
    "# Store the data in the model_comparison table\n",
    "models = {'Model':['Support Vector Classifier'], 'Scoring Metric':'Accuracy', 'Scoring Value':[cv.mean()]}\n",
    "model_comparison = model_comparison.append(pd.DataFrame(data=models, columns=models.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for many combinations of C, gamma, and kernel.\n",
    "\n",
    "All params default: Average Accuracy (AA) = 0.524266973886\n",
    "\n",
    "C = .25, other params default: AA = 0.524266973886\n",
    "\n",
    "C = .75, other params default: AA = 0.524266973886\n",
    "\n",
    "Kernel = 'linear,' other params default: AA = Would not run\n",
    "\n",
    "Kernel = 'poly,' 'sigmoid,' or 'precomputed,' other params default: ----> 9 svc.fit(X_train,Y_train) TypeError: must be real number, not str\n",
    "\n",
    "Gamma = .1, other params default: AA = 0.524266973886\n",
    "\n",
    "Gamma = .9, other params default: AA = 0.524266973886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_pred.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 500 iterations, using 5-deep trees, and loss function 'deviance.'\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 5,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train,Y_train)\n",
    "y_pred = clf.fit(X_train,Y_train).predict(X_test)\n",
    "\n",
    "print('Confusion Matrix of the model:')\n",
    "conf_mat = confusion_matrix(Y_test, y_pred)\n",
    "print(conf_mat)\n",
    "\n",
    "# Print the AUROC.\n",
    "cv = cross_val_score(clf, X, Y, cv=10, scoring='accuracy') \n",
    "print('Accuracy Score:')\n",
    "print(cv)\n",
    "# Print the average of the AUROC and store it in the table\n",
    "print('Average of the Accuracy Score:')\n",
    "print(cv.mean())\n",
    "\n",
    "# Store the data in the model_comparison table\n",
    "models = {'Model':['Gradient Boosting Classifier'], 'Scoring Metric':'Accuracy', 'Scoring Value':[cv.mean()]}\n",
    "model_comparison = model_comparison.append(pd.DataFrame(data=models, columns=models.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_pred.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Selection and Analysis of the Best Performing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Being provided one thousand dollars cash every ten business days, how will the top three models perform compared to one another as well as to a 401(k) approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a cash_available list that stores how much cash is available \n",
    "# to buy the stocks. It will be $1000 every ten business days.\n",
    "cash_available = [0]\n",
    "\n",
    "# Start out with 0 stocks_owned\n",
    "stocks_owned = [0]\n",
    "\n",
    "# Create DataFrame for profits\n",
    "profits = {'Model':[],'Profit':[]}\n",
    "columns = profits.keys()\n",
    "model_profits = pd.DataFrame(data=profits, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a pred_return function to calculate the returns of the prediction models\n",
    "def pred_return(y_pred, data, cash_available, stocks_owned):\n",
    "    # Create DataFrame for values\n",
    "    new_values = {'Stocks':[],'Cash':[]}\n",
    "    columns = new_values.keys()\n",
    "    values = pd.DataFrame(data=new_values, columns=columns)\n",
    "    for i in range(len(y_pred)):\n",
    "        # For every tenth iteration of i, add 1000 to cash_available\n",
    "        if i%10 == 0:\n",
    "            cash_available = cash_available+1000\n",
    "        # If the predicted value is greater than zero, buy more stock\n",
    "        if y_pred[i] > 0:\n",
    "            [stocks_owned,cash_available] = buy_stock(cash_available, \n",
    "                                                      stocks_owned, y_pred, \n",
    "                                                      data.Close[i])\n",
    "        # If the predicted value is less than zero, sell stock\n",
    "        elif y_pred[i] < 0:\n",
    "            [stocks_owned,cash_available] = sell_stock(cash_available, \n",
    "                                                      stocks_owned, y_pred, \n",
    "                                                      data.Close[i])\n",
    "        stocks_owned = [stocks_owned,cash_available][0]\n",
    "        cash_available = [stocks_owned,cash_available][1]\n",
    "        new_values = {'Stocks':[stocks_owned], 'Cash':[cash_available]}\n",
    "        values = values.append(pd.DataFrame(data=new_values, columns=\n",
    "                                            new_values.keys()), \n",
    "                                            ignore_index=True)\n",
    "    #print(values,y_pred)\n",
    "    return(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a buy_stock function that buys as many stocks as can be afforded\n",
    "def buy_stock(cash_available, stocks_owned, y_pred, value):\n",
    "    # Set number of stocks to buy\n",
    "    num_stocks_buy = int(cash_available/value)\n",
    "    # Subtract from cash_available, store it in a list\n",
    "    cash_available = cash_available-num_stocks_buy*value\n",
    "    stocks_owned = stocks_owned+num_stocks_buy\n",
    "    return(stocks_owned, cash_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a sell_stock function that sells all stocks\n",
    "def sell_stock(cash_available, stocks_owned, y_pred, value):\n",
    "    sell_value = stocks_owned*value\n",
    "    cash_available = cash_available + sell_value\n",
    "    stocks_owned = 0\n",
    "    return(stocks_owned, cash_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the 401k approach simulator, safe_invest, \n",
    "# That buys as much stock as is available every two weeks / ten business days.\n",
    "def safe_invest(data, cash_available, stocks_owned):\n",
    "    # Create DataFrame for values\n",
    "    new_values = {'Stocks':[],'Cash':[]}\n",
    "    columns = new_values.keys()\n",
    "    values = pd.DataFrame(data=new_values, columns=columns)\n",
    "    for i in range(len(data.loc[offset:,:])):\n",
    "        # For every tenth iteration of i, add 1000 to cash_available\n",
    "        if i%10 == 0:\n",
    "            cash_available = cash_available+1000\n",
    "        num_stocks_buy = int(cash_available/data.Close[i])\n",
    "        stocks_owned = num_stocks_buy + stocks_owned\n",
    "        cash_available = cash_available-num_stocks_buy*data.Close[i]\n",
    "        # Store the values in a table\n",
    "        new_values = {'Stocks':[stocks_owned], 'Cash':[cash_available]}\n",
    "        values = values.append(pd.DataFrame(data=new_values, columns=\n",
    "                                            new_values.keys()), \n",
    "                                            ignore_index=True)\n",
    "\n",
    "    #print(values)\n",
    "    return(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the pred_return function for lr\n",
    "y_pred = lr.fit(X_train,Y_train).predict(X_test)\n",
    "cash_available = 0\n",
    "stocks_owned = 0\n",
    "values = pred_return(y_pred, data, cash_available, stocks_owned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the ending returns from the lr model\n",
    "lr_returns = values.loc[len(values)-1,'Stocks']*data.loc[len(y_pred)-1,'Close'] + values.loc[len(values)-1, 'Cash']\n",
    "print(lr_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1257 rows of data\n",
    "# $1000 granted every 10 days = 125,700 dollars granted.\n",
    "# Since we are predicting the last 20% of the data, multiply by .20 \n",
    "# A total of $25,140 granted in the last 20% of the data\n",
    "# Get total profit by subrtracting returns by dollars granted. \n",
    "# Get total profit by subrtracting returns by dollars granted.  \n",
    "lr_profit = lr_returns - 25140\n",
    "print(lr_profit)\n",
    "\n",
    "# Store the profit in the model_profits table\n",
    "profits = {'Model':['Logistic Regression'], 'Profit':lr_profit}\n",
    "model_profits = model_profits.append(pd.DataFrame(data=profits, columns=profits.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the pred_return function for ridge\n",
    "y_pred = ridgeregr.fit(X_train,Y_train).predict(X_test)\n",
    "cash_available = 0\n",
    "stocks_owned = 0\n",
    "values = pred_return(y_pred, data, cash_available, stocks_owned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the ending returns from the lr model\n",
    "ridgeregr_returns = values.loc[len(values)-1,'Stocks']*data.loc[len(y_pred)-1,'Close'] + values.loc[len(values)-1, 'Cash']\n",
    "print(ridgeregr_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1257 rows of data\n",
    "# $1000 granted every 10 days = 125,700 dollars granted.\n",
    "# Since we are predicting the last 20% of the data, multiply by .20 \n",
    "# A total of $25,140 granted in the last 20% of the data\n",
    "# Get total profit by subrtracting returns by dollars granted. \n",
    "# Get total profit by subrtracting returns by dollars granted.  \n",
    "ridgeregr_profit = ridgeregr_returns - 25140\n",
    "print(ridgeregr_profit)\n",
    "\n",
    "# Store the profit in the model_profits table\n",
    "profits = {'Model':['Ridge Regression'], 'Profit':ridgeregr_profit}\n",
    "model_profits = model_profits.append(pd.DataFrame(data=profits, columns=profits.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the pred_return function for lasso\n",
    "y_pred = lasso.fit(X_train,Y_train).predict(X_test)\n",
    "cash_available = 0\n",
    "stocks_owned = 0\n",
    "values = pred_return(y_pred, data, cash_available, stocks_owned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the ending returns from the lr model\n",
    "lasso_returns = values.loc[len(values)-1,'Stocks']*data.loc[len(y_pred)-1,'Close'] + values.loc[len(values)-1, 'Cash']\n",
    "print(lasso_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1257 rows of data\n",
    "# $1000 granted every 10 days = 125,700 dollars granted.\n",
    "# Since we are predicting the last 20% of the data, multiply by .20 \n",
    "# A total of $25,140 granted in the last 20% of the data\n",
    "# Get total profit by subrtracting returns by dollars granted. \n",
    "# Get total profit by subrtracting returns by dollars granted.  \n",
    "lasso_profit = lasso_returns - 25140\n",
    "print(lasso_profit)\n",
    "\n",
    "# Store the profit in the model_profits table\n",
    "profits = {'Model':['Lasso Regression'], 'Profit':lasso_profit}\n",
    "model_profits = model_profits.append(pd.DataFrame(data=profits, columns=profits.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the pred_return function for svr\n",
    "y_pred = svr.fit(X_train,Y_train).predict(X_test)\n",
    "cash_available = 0\n",
    "stocks_owned = 0\n",
    "values = pred_return(y_pred, data, cash_available, stocks_owned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the ending returns from the svr model\n",
    "svr_returns = values.loc[len(values)-1,'Stocks']*data.loc[len(y_pred)-1,'Close'] + values.loc[len(values)-1, 'Cash']\n",
    "print(svr_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1257 rows of data\n",
    "# $1000 granted every 10 days = 125,700 dollars granted.\n",
    "# Since we are predicting the last 20% of the data, multiply by .20 \n",
    "# A total of $25,140 granted in the last 20% of the data\n",
    "# Get total profit by subrtracting returns by dollars granted. \n",
    "# Get total profit by subrtracting returns by dollars granted.  \n",
    "svr_profit = svr_returns - 25140\n",
    "print(svr_profit)\n",
    "\n",
    "# Store the profit in the model_profits table\n",
    "profits = {'Model':['Support Vector Regression'], 'Profit':svr_profit}\n",
    "model_profits = model_profits.append(pd.DataFrame(data=profits, columns=profits.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the pred_return function for svc\n",
    "y_pred = svc.fit(X_train,Y_train).predict(X_test)\n",
    "cash_available = 0\n",
    "stocks_owned = 0\n",
    "values = pred_return(y_pred, data, cash_available, stocks_owned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the ending returns from the svc model\n",
    "svc_returns = values.loc[len(values)-1,'Stocks']*data.loc[len(y_pred)-1,'Close'] + values.loc[len(values)-1, 'Cash']\n",
    "print(svc_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1257 rows of data\n",
    "# $1000 granted every 10 days = 125,700 dollars granted.\n",
    "# Since we are predicting the last 20% of the data, multiply by .20 \n",
    "# A total of $25,140 granted in the last 20% of the data\n",
    "# Get total profit by subrtracting returns by dollars granted. \n",
    "# Get total profit by subrtracting returns by dollars granted.  \n",
    "svc_profit = svc_returns - 25140\n",
    "print(svc_profit)\n",
    "\n",
    "# Store the profit in the model_profits table\n",
    "profits = {'Model':['Support Vector Classifier'], 'Profit':svc_profit}\n",
    "model_profits = model_profits.append(pd.DataFrame(data=profits, columns=profits.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the pred_return function for gradient boosting classifier\n",
    "y_pred = clf.fit(X_train,Y_train).predict(X_test)\n",
    "cash_available = 0\n",
    "stocks_owned = 0\n",
    "values = pred_return(y_pred, data, cash_available, stocks_owned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the ending returns from the clf model\n",
    "clf_returns = values.loc[len(values)-1,'Stocks']*data.loc[len(y_pred)-1,'Close'] + values.loc[len(values)-1, 'Cash']\n",
    "print(clf_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1257 rows of data\n",
    "# $1000 granted every 10 days = 125,700 dollars granted.\n",
    "# Since we are predicting the last 20% of the data, multiply by .20 \n",
    "# A total of $25,140 granted in the last 20% of the data\n",
    "# Get total profit by subrtracting returns by dollars granted. \n",
    "# Get total profit by subrtracting returns by dollars granted.  \n",
    "clf_profit = clf_returns - 25140\n",
    "print(clf_profit)\n",
    "\n",
    "# Store the profit in the model_profits table\n",
    "profits = {'Model':['Gradient Boosting Classifier'], 'Profit':clf_profit}\n",
    "model_profits = model_profits.append(pd.DataFrame(data=profits, columns=profits.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the safe_invest function\n",
    "cash_available = 0\n",
    "stocks_owned = 0\n",
    "values = safe_invest(data, cash_available, stocks_owned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the ending returns from the safe_invest function\n",
    "safe_returns = values.loc[len(values)-1,'Stocks']*data.loc[len(data.loc[offset:,:])-1,'Close'] + values.loc[len(values)-1, 'Cash']\n",
    "print(safe_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1257 rows of data\n",
    "# $1000 granted every 10 days = 125,700 dollars granted.\n",
    "# Since we are predicting the last 20% of the data, multiply by .20 \n",
    "# A total of $25,140 granted in the last 20% of the data\n",
    "# Get total profit by subrtracting returns by dollars granted. \n",
    "# Get total profit by subrtracting returns by dollars granted.  \n",
    "safe_profit = safe_returns - 25140\n",
    "print(safe_profit)\n",
    "\n",
    "# Store the profit in the model_profits table\n",
    "profits = {'Model':['401k Simulator'], 'Profit':safe_profit}\n",
    "model_profits = model_profits.append(pd.DataFrame(data=profits, columns=profits.keys()), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_profits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis:\n",
    "\n",
    "The test data had an average momentum of 0.07142857142857142, which correlates to the overall growth of the stock over the test data. This average will be compared to the average predicted momentums (average of y_pred) for each model. \n",
    "\n",
    "Logistic Regression and Support Vector Classifier had an average y_pred of 1.0, meaning that the model predicted an upward movement for every point, and therefore acted in the same manner as the 401(k) simulator. \n",
    "\n",
    "The Gradient Boosting Classifier predicted rises and falls in the stock value, with an overall average of -0.166666666667. This shows that the model behaved appropriately in that it predicted rises and falls, but it did not perform as well as the 401(k) simulator. The results make sense, since the Gradient Boosting Classifier had an average accuracy of .50, whereas the Logistic Regression and Support Vector Classifiers had average accuries of .52.\n",
    "\n",
    "Lasso Regression and Support Vector Regression had average y_pred values of 0.0228357109921 and 0.0213930348259, respectively. This shows that these two models behaved appropriately in that they predicted rises and falls, and they actually generated a profit that was equal to the 401(k) simulator. \n",
    "\n",
    "# What happened with Ridge Regression?\n",
    "\n",
    "Ridge Regression had an average y_pred of 0.0248827351445. This shows that the model performed appropriately in that it predicted the rises and fals of the stock. The model returned a higher profit than the 401(k) simulator.\n",
    "\n",
    "Ridge Regression performed better than Lasso Regression because Ridge shrinks the higher explanatory feature rather than dropping groups of features (which is what Lasso does). \n",
    "\n",
    "There is some multicollinearity caused by adding the streak feature (it has a high correlation with momentum), but it was kept.\n",
    "\n",
    "Note that Ridge and Lasso Regression both work to optimize the variance explained in the test set.\n",
    "\n",
    "# Future Work\n",
    "\n",
    "The models could be improved upon by creating additional features such as the 52-week moving average as well as by implementing some Natural Language Processing to parse the web for sentiment analysis of stocks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
